{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffered Leave one Group Out Cross Validation\n",
    "\n",
    "This Jupyter notebook runs buffered leave-one-group-out cross-validation stratified by mean annual surface air temperature and mean annual precipitation67. This tests the model’s ability to predict CWMs for temperature and precipitation values that have been removed from its training set. For example, if a test group has a mean temperature of 10°C and a 2°C buffer is applied, then all training data with temperatures between 9°C and 11°C are excluded from the model training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename CHELSA_bio columns consistently\n",
    "def rename_chelsa_columns(df):\n",
    "    def clean_chelsa_name(col):\n",
    "        if col.startswith('CHELSA_bio'):\n",
    "            parts = col.split('_')\n",
    "            if len(parts) > 1 and parts[1].startswith('bio'):\n",
    "                bio_number = parts[1][3:]  # Extract number part from 'bio<number>'\n",
    "                return f'CHELSA_bio{bio_number}'\n",
    "        return col\n",
    "\n",
    "    # Apply the renaming function to all columns\n",
    "    df.columns = [clean_chelsa_name(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# Function to get a consistent numerical order for CHELSA_bio columns\n",
    "def get_consistent_chelsa_order(*dfs):\n",
    "    # Collect all CHELSA_bio columns from all DataFrames\n",
    "    all_chelsa_columns = set()\n",
    "    for df in dfs:\n",
    "        chelsa_columns = [col for col in df.columns if col.startswith('CHELSA_bio')]\n",
    "        all_chelsa_columns.update(chelsa_columns)\n",
    "    \n",
    "    # Sort the collected columns numerically\n",
    "    sorted_chelsa_columns = sorted(all_chelsa_columns, key=lambda x: int(x.replace('CHELSA_bio', '')))\n",
    "    return sorted_chelsa_columns\n",
    "\n",
    "# Function to reorder columns based on a given order\n",
    "def reorder_columns(df, desired_order):\n",
    "    chelsa_columns = [col for col in df.columns if col.startswith('CHELSA_bio')]\n",
    "    non_chelsa_columns = [col for col in df.columns if not col.startswith('CHELSA_bio')]\n",
    "    \n",
    "    # Ensure that only the columns that exist in the DataFrame are reordered\n",
    "    chelsa_columns = [col for col in desired_order if col in chelsa_columns]\n",
    "    \n",
    "    # Reorder columns\n",
    "    ordered_columns = non_chelsa_columns + chelsa_columns\n",
    "    return df[ordered_columns]\n",
    "\n",
    "# Function to move 'lat' and 'lon' to specific positions\n",
    "def move_lat_lon_columns(df, lat_pos=None, lon_pos=None):\n",
    "    columns = df.columns.tolist()  # Get the list of columns\n",
    "    if 'lat' in columns and lat_pos is not None:\n",
    "        columns.insert(lat_pos, columns.pop(columns.index('lat')))  # Move 'lat' to the specified position\n",
    "    if 'lon' in columns and lon_pos is not None:\n",
    "        columns.insert(lon_pos, columns.pop(columns.index('lon')))  # Move 'lon' to the specified position\n",
    "    return df[columns]  # Reorder DataFrame\n",
    "\n",
    "# Load and rename columns for all DataFrames\n",
    "current_df = pd.read_csv('data/precomputed/plot_and_abiotic_data_current.csv')\n",
    "current_df = rename_chelsa_columns(current_df)\n",
    "\n",
    "future_climate_ssp126 = pd.read_csv('data/precomputed/plot_and_abiotic_data_ssp126.csv')\n",
    "future_climate_ssp126 = rename_chelsa_columns(future_climate_ssp126)\n",
    "\n",
    "future_climate_ssp370 = pd.read_csv('data/precomputed/plot_and_abiotic_data_ssp370.csv')\n",
    "future_climate_ssp370 = rename_chelsa_columns(future_climate_ssp370)\n",
    "\n",
    "future_climate_ssp585 = pd.read_csv('data/precomputed/plot_and_abiotic_data_ssp585.csv')\n",
    "future_climate_ssp585 = rename_chelsa_columns(future_climate_ssp585)\n",
    "future_climate_ssp585 = rename_chelsa_columns(future_climate_ssp585)\n",
    "\n",
    "# Determine the consistent CHELSA_bio column order\n",
    "desired_order = get_consistent_chelsa_order(current_df, future_climate_ssp126, future_climate_ssp370, future_climate_ssp585)\n",
    "\n",
    "# Apply the same column order to all DataFrames\n",
    "current_df = reorder_columns(current_df, desired_order)\n",
    "future_climate_ssp126 = reorder_columns(future_climate_ssp126, desired_order)\n",
    "future_climate_ssp370 = reorder_columns(future_climate_ssp370, desired_order)\n",
    "future_climate_ssp585 = reorder_columns(future_climate_ssp585, desired_order)\n",
    "\n",
    "# Move 'lat' and 'lon' columns to specific positions\n",
    "current_df = move_lat_lon_columns(current_df, lat_pos=26, lon_pos=27)\n",
    "future_climate_ssp126 = move_lat_lon_columns(future_climate_ssp126, lat_pos=1, lon_pos=2)  \n",
    "future_climate_ssp370 = move_lat_lon_columns(future_climate_ssp370, lat_pos=1, lon_pos=2)  \n",
    "future_climate_ssp585 = move_lat_lon_columns(future_climate_ssp585, lat_pos=1, lon_pos=2)  \n",
    "\n",
    "current_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precipitation LOGOCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "trait_names = current_df.columns[1:25].tolist()\n",
    "\n",
    "# Define climate features\n",
    "climate_features = [\n",
    "    'EarthEnvTopoMed_Eastness', 'EarthEnvTopoMed_Elevation',\n",
    "    'EarthEnvTopoMed_Northness', 'EarthEnvTopoMed_Slope',\n",
    "    'SG_Bulk_density_015cm', 'SG_Clay_Content_015cm',\n",
    "    'SG_Coarse_fragments_015cm', 'SG_Depth_to_bedrock',\n",
    "    'SG_Sand_Content_015cm', 'SG_Silt_Content_015cm', 'CHELSA_bio1',\n",
    "    'CHELSA_bio7', 'CHELSA_bio9', 'CHELSA_bio11', 'CHELSA_bio12',\n",
    "    'CHELSA_bio15', 'CHELSA_bio17', 'CHELSA_bio19'\n",
    "]\n",
    "\n",
    "# Define precipitation buffer sizes\n",
    "group_sizes_pr = np.arange(0, 160, 20)\n",
    "bins_pr = np.arange(150, 6600, 20)\n",
    "\n",
    "# Drop NaNs and reassign precipitation groups\n",
    "current_df = current_df.dropna(subset=trait_names + climate_features)\n",
    "current_df['precipitation_group_pr'] = pd.cut(current_df['CHELSA_bio12'], bins_pr, labels=bins_pr[:-1], include_lowest=True)\n",
    "current_df = current_df.dropna(subset=['precipitation_group_pr'])\n",
    "precipitation_labels_pr = current_df['precipitation_group_pr'].astype(np.int16).to_numpy()\n",
    "\n",
    "# Extract features and targets\n",
    "features = current_df[climate_features].values\n",
    "targets = current_df.iloc[:, 1:len(trait_names) + 1].values\n",
    "\n",
    "# Scale features and targets\n",
    "scaler_features = StandardScaler()\n",
    "scaler_targets = StandardScaler()\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "targets_scaled = scaler_targets.fit_transform(targets)\n",
    "\n",
    "assert features_scaled.shape[0] == targets_scaled.shape[0] == len(precipitation_labels_pr)\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "features_poly = poly.fit_transform(features_scaled)\n",
    "\n",
    "# Initialise Leave-One-Group-Out CV\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Store all predictions and true values\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Perform LOGOCV\n",
    "total_test_sets = 0\n",
    "for group_size_pr in group_sizes_pr:\n",
    "    print(f\"\\nCurrent group size: {group_size_pr} mm\")\n",
    "    \n",
    "    for train_index, test_index in logo.split(features_poly, targets_scaled, groups=precipitation_labels_pr):\n",
    "        test_group_index = precipitation_labels_pr[test_index[0]]\n",
    "        if not (190 <= test_group_index <= 2010):\n",
    "            continue\n",
    "        \n",
    "        exclude_groups = np.arange(test_group_index - group_size_pr, test_group_index + group_size_pr + 20, 20)\n",
    "        train_mask = ~np.isin(precipitation_labels_pr, exclude_groups)\n",
    "        \n",
    "        features_train, features_test = features_poly[train_mask], features_poly[test_index]\n",
    "        target_train, target_test = targets_scaled[train_mask], targets_scaled[test_index]\n",
    "        \n",
    "        print(f\"Test group index: {test_group_index}, Exclude groups: [{exclude_groups}]\")\n",
    "        print(f\"Number of training samples: {len(features_train)}, Number of test samples: {len(features_test)}\")\n",
    "        \n",
    "        if len(features_train) == 0 or len(features_test) == 0:\n",
    "            continue\n",
    "        \n",
    "        rf_regressor = RandomForestRegressor(n_estimators=1800, random_state=42, min_samples_split=2, \n",
    "                                             min_samples_leaf=1, max_features='log2', max_depth=20, n_jobs=-1, bootstrap=False)\n",
    "        multioutput_rf = MultiOutputRegressor(rf_regressor)\n",
    "        multioutput_rf.fit(features_train, target_train)\n",
    "        \n",
    "        rf_pred = multioutput_rf.predict(features_test)\n",
    "        \n",
    "        # Store results\n",
    "        all_y_true.append(target_test)\n",
    "        all_y_pred.append(rf_pred)\n",
    "        \n",
    "        # Compute per-test-set R squared and NRMSE\n",
    "        r2_test = r2_score(target_test.ravel(), rf_pred.ravel())\n",
    "        rmse_test = np.sqrt(mean_squared_error(target_test.ravel(), rf_pred.ravel()))\n",
    "        nrmse_test = rmse_test / (np.max(target_test.ravel()) - np.min(target_test.ravel()))\n",
    "        \n",
    "        print(f\"Test set R squared: {r2_test:.4f}, NRMSE: {nrmse_test:.4f}\")\n",
    "        total_test_sets += 1\n",
    "    \n",
    "# Concatenate all stored predictions and true values\n",
    "all_y_true = np.vstack(all_y_true)\n",
    "all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "# Compute overall R squared and NRMSE\n",
    "final_r2 = r2_score(all_y_true.ravel(), all_y_pred.ravel())\n",
    "final_rmse = np.sqrt(mean_squared_error(all_y_true.ravel(), all_y_pred.ravel()))\n",
    "final_nrmse = final_rmse / (np.max(all_y_true.ravel()) - np.min(all_y_true.ravel()))\n",
    "\n",
    "print(f\"\\nFinal Overall R squared: {final_r2:.4f}\")\n",
    "print(f\"Final Overall NRMSE: {final_nrmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logocv_pr = pd.read_csv(\"data/precomputed/logocv_results_pr.csv\")\n",
    "logocv_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SSP precipitation ranges relative to the baseline\n",
    "ssp_scenarios_pr = ['SSP126', 'SSP370', 'SSP585']\n",
    "ssp_means_pr = [49.94, 64.05, 78.62]  # Mean pr increase relative to baseline\n",
    "lower_confidence_pr = [3.59, 26.02, 4.37]  # Lower bound of 90% confidence interval. Absolute of negative values used\n",
    "upper_confidence_pr = [97.5, 167.46, 185.79]  # Upper bound of 90% confidence interval. Absolute of negative values used\n",
    "\n",
    "# Plotting overall R2 and NRMSE against degrees Celsius buffering\n",
    "plt.figure(figsize=(10, 6), dpi=600)\n",
    "\n",
    "# Plot R2 and NRMSE\n",
    "plt.plot(logocv_pr.buffer_size, logocv_pr.R2, marker='o', linestyle='-', color='b', label='Weighted R-squared')\n",
    "plt.plot(logocv_pr.buffer_size, logocv_pr.NRMSE, marker='s', linestyle='--', color='r', label='Weighted NRMSE')\n",
    "\n",
    "# Define colors in a gradient from light green to light red\n",
    "colors = ['lightgreen', 'orange', 'lightcoral']\n",
    "\n",
    "# Plot SSP scenarios as shaded regions with labels and arrows\n",
    "for ssp, mean, lower, upper, color in zip(ssp_scenarios_pr, ssp_means_pr, lower_confidence_pr, upper_confidence_pr, colors):\n",
    "    plt.fill_betweenx([0, 1], lower, upper, color=color, alpha=0.25, edgecolor='black', linewidth=0.5, zorder=1)\n",
    "    plt.annotate(ssp, \n",
    "                 xy=(mean, 0.9 - ssp_scenarios_pr.index(ssp) * 0.1), \n",
    "                 xytext=(0, -10), textcoords='offset points',\n",
    "                 ha='center', va='top', fontsize=16, color='black', zorder=2)\n",
    "    plt.annotate('', \n",
    "                 xy=(lower, 0.9 - ssp_scenarios_pr.index(ssp) * 0.1), \n",
    "                 xytext=(upper, 0.9 - ssp_scenarios_pr.index(ssp) * 0.1),\n",
    "                 arrowprops=dict(arrowstyle='<->', color='black'), zorder=2)\n",
    "    # Adding small vertical line for mean precipitation within the range\n",
    "    plt.plot([mean, mean], [0.9 - ssp_scenarios_pr.index(ssp) * 0.1 - 0.02, 0.9 - ssp_scenarios_pr.index(ssp) * 0.1 + 0.02],\n",
    "             color='black', linestyle='-', linewidth=0.5, zorder=3)\n",
    "\n",
    "# Annotate each point with its value for R2 and NRMSE\n",
    "for i, txt in enumerate(logocv_pr.R2):\n",
    "    plt.annotate(f\"{txt:.2f}\", (logocv_pr.buffer_size[i], logocv_pr.R2[i]),\n",
    "                 textcoords=\"offset points\", xytext=(0, 10), ha='center', va='bottom', color='b', fontsize=16)\n",
    "\n",
    "for i, txt in enumerate(logocv_pr.NRMSE):\n",
    "    plt.annotate(f\"{txt:.2f}\", (logocv_pr.buffer_size[i], logocv_pr.NRMSE[i]),\n",
    "                 textcoords=\"offset points\", xytext=(0, 10), ha='center', color='r', fontsize=16)\n",
    "\n",
    "# Set axes range and tick increments\n",
    "plt.xticks(np.arange(0, max(logocv_pr.buffer_size) + 40, 40), fontsize=14)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.yticks(np.arange(0, 1.1, 0.2), fontsize=16)\n",
    "\n",
    "# Legend placement in the top right corner\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "plt.xlabel('Precipitation (mm) buffering', fontsize=16)\n",
    "plt.ylabel('Coefficient of determination (R2)', fontsize=16)\n",
    "plt.title('', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperature LOGOCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature: Buffered LOGOCV using incremental 0.5± degrees celsius buffer. Metrics weighted by group size\n",
    "\n",
    "# Load dataset\n",
    "trait_names = current_df.columns[1:25].tolist()\n",
    "\n",
    "# Define climate features\n",
    "climate_features = [\n",
    "    'EarthEnvTopoMed_Eastness', 'EarthEnvTopoMed_Elevation',\n",
    "    'EarthEnvTopoMed_Northness', 'EarthEnvTopoMed_Slope',\n",
    "    'SG_Bulk_density_015cm', 'SG_Clay_Content_015cm',\n",
    "    'SG_Coarse_fragments_015cm', 'SG_Depth_to_bedrock',\n",
    "    'SG_Sand_Content_015cm', 'SG_Silt_Content_015cm', 'CHELSA_bio1',\n",
    "    'CHELSA_bio7', 'CHELSA_bio9', 'CHELSA_bio11', 'CHELSA_bio12',\n",
    "    'CHELSA_bio15', 'CHELSA_bio17', 'CHELSA_bio19'\n",
    "]\n",
    "\n",
    "# Define bins and labels for temperature groups\n",
    "group_sizes = np.arange(0, 4, 0.5)  # Group sizes in degrees Celsius to exclude\n",
    "bins = np.arange(-9, 25, 0.5)  # Create bins from -7 to +10 with 0.5 degree intervals\n",
    "labels = np.arange(-9, 24.5, 0.5)  # Create labels corresponding to the bins\n",
    "\n",
    "# Drop NaNs and reassign temp groups\n",
    "current_df = current_df.dropna(subset=trait_names + climate_features)\n",
    "current_df['temperature_group'] = pd.cut(current_df['CHELSA_bio1'], bins, labels=bins[:-1], include_lowest=True)\n",
    "current_df = current_df.dropna(subset=['temperature_group'])\n",
    "temperature_labels = current_df['temperature_group'].astype(np.float32).to_numpy()\n",
    "\n",
    "# Extract features and targets\n",
    "features = current_df[climate_features].values\n",
    "targets = current_df.iloc[:, 1:len(trait_names) + 1].values\n",
    "\n",
    "# Scale features and targets\n",
    "scaler_features = StandardScaler()\n",
    "scaler_targets = StandardScaler()\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "targets_scaled = scaler_targets.fit_transform(targets)\n",
    "\n",
    "assert features_scaled.shape[0] == targets_scaled.shape[0] == len(temperature_labels)\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "features_poly = poly.fit_transform(features_scaled)\n",
    "\n",
    "# Initialise Leave-One-Group-Out CV\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Store all predictions and true values\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Perform LOGOCV\n",
    "total_test_sets = 0\n",
    "for group_size in group_sizes:\n",
    "    print(f\"\\nCurrent group size: {group_size} C\")\n",
    "    \n",
    "    for train_index, test_index in logo.split(features_poly, targets_scaled, groups=temperature_labels):\n",
    "        test_group_index = temperature_labels[test_index[0]]\n",
    "            \n",
    "        exclude_groups = np.arange(test_group_index - group_size, test_group_index + group_size + 0.5, 0.5)\n",
    "        train_mask = ~np.isin(temperature_labels, exclude_groups)\n",
    "        \n",
    "        features_train, features_test = features_poly[train_mask], features_poly[test_index]\n",
    "        target_train, target_test = targets_scaled[train_mask], targets_scaled[test_index]\n",
    "        \n",
    "        print(f\"Test group index: {test_group_index}, Exclude groups: [{exclude_groups}]\")\n",
    "        print(f\"Number of training samples: {len(features_train)}, Number of test samples: {len(features_test)}\")\n",
    "        \n",
    "        if len(features_train) == 0 or len(features_test) == 0:\n",
    "            continue\n",
    "        \n",
    "        rf_regressor = RandomForestRegressor(n_estimators=1800, random_state=42, min_samples_split=2, \n",
    "                                             min_samples_leaf=1, max_features='log2', max_depth=20, n_jobs=-1, bootstrap=False)\n",
    "        multioutput_rf = MultiOutputRegressor(rf_regressor)\n",
    "        multioutput_rf.fit(features_train, target_train)\n",
    "        \n",
    "        rf_pred = multioutput_rf.predict(features_test)\n",
    "        \n",
    "        # Store results\n",
    "        all_y_true.append(target_test)\n",
    "        all_y_pred.append(rf_pred)\n",
    "        \n",
    "        # Compute per-test-set R squared and NRMSE\n",
    "        r2_test = r2_score(target_test.ravel(), rf_pred.ravel())\n",
    "        rmse_test = np.sqrt(mean_squared_error(target_test.ravel(), rf_pred.ravel()))\n",
    "        nrmse_test = rmse_test / (np.max(target_test.ravel()) - np.min(target_test.ravel()))\n",
    "        \n",
    "        print(f\"Test set R squared: {r2_test:.4f}, NRMSE: {nrmse_test:.4f}\")\n",
    "        total_test_sets += 1\n",
    "    \n",
    "# Concatenate all stored predictions and true values\n",
    "all_y_true = np.vstack(all_y_true)\n",
    "all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "# Compute overall R squared and NRMSE\n",
    "final_r2 = r2_score(all_y_true.ravel(), all_y_pred.ravel())\n",
    "final_rmse = np.sqrt(mean_squared_error(all_y_true.ravel(), all_y_pred.ravel()))\n",
    "final_nrmse = final_rmse / (np.max(all_y_true.ravel()) - np.min(all_y_true.ravel()))\n",
    "\n",
    "print(f\"\\nFinal Overall R squared: {final_r2:.4f}\")\n",
    "print(f\"Final Overall NRMSE: {final_nrmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logocv_temp = pd.read_csv(\"data/logocv_results_temp.csv\")\n",
    "logocv_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SSP temperature ranges relative to the baseline\n",
    "ssp_scenarios = ['SSP126', 'SSP370', 'SSP585']\n",
    "ssp_means = [1.59, 4.1, 5.47]  # Mean temperature increase relative to baseline\n",
    "lower_confidence = [1.0, 3.09, 4.01]  # Lower bound of 90% confidence interval\n",
    "upper_confidence = [2.63, 5.72, 7.29]  # Upper bound of 90% confidence interval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plotting overall R2 and NRMSE against degrees Celsius buffering\n",
    "plt.figure(figsize=(10, 6), dpi=600)\n",
    "\n",
    "# Plot R2 and NRMSE\n",
    "plt.plot(logocv_temp.buffer_size, logocv_temp.R2, marker='o', linestyle='-', color='b', label='Weighted R-squared')\n",
    "plt.plot(logocv_temp.buffer_size, logocv_temp.NRMSE, marker='s', linestyle='--', color='r', label='Weighted NRMSE')\n",
    "\n",
    "# Define colors in a gradient from light green to light red\n",
    "colors = ['lightgreen', 'orange', 'lightcoral']\n",
    "\n",
    "# Plot SSP scenarios as shaded regions with labels and arrows\n",
    "for ssp, mean, lower, upper, color in zip(ssp_scenarios, ssp_means, lower_confidence, upper_confidence, colors):\n",
    "    plt.fill_betweenx([0, 1], lower, upper, color=color, alpha=0.25, edgecolor='black', linewidth=0.5, zorder=1)\n",
    "    plt.annotate(\n",
    "        ssp, \n",
    "        xy=(mean, 0.9 - ssp_scenarios.index(ssp) * 0.1), \n",
    "        xytext=(0, -10), \n",
    "        textcoords='offset points',\n",
    "        ha='center', \n",
    "        va='top', \n",
    "        fontsize=16, \n",
    "        color='black', \n",
    "        zorder=2\n",
    "    )\n",
    "    plt.annotate(\n",
    "        '', \n",
    "        xy=(lower, 0.9 - ssp_scenarios.index(ssp) * 0.1), \n",
    "        xytext=(upper, 0.9 - ssp_scenarios.index(ssp) * 0.1),\n",
    "        arrowprops=dict(arrowstyle='<->', color='black'),\n",
    "        zorder=2\n",
    "    )\n",
    "    # Adding a small vertical line for the mean temperature within the range\n",
    "    plt.plot(\n",
    "        [mean, mean], \n",
    "        [0.9 - ssp_scenarios.index(ssp) * 0.1 - 0.02, 0.9 - ssp_scenarios.index(ssp) * 0.1 + 0.02],\n",
    "        color='black', \n",
    "        linestyle='-', \n",
    "        linewidth=0.5, \n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "# Annotate each point with its value for R2 and NRMSE\n",
    "for i, txt in enumerate(logocv_temp.R2):\n",
    "    plt.annotate(\n",
    "        f\"{txt:.2f}\", \n",
    "        (logocv_temp.buffer_size[i], logocv_temp.R2[i]), \n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10), \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        color='b', \n",
    "        fontsize=16\n",
    "    )\n",
    "\n",
    "for i, txt in enumerate(logocv_temp.NRMSE):\n",
    "    plt.annotate(\n",
    "        f\"{txt:.2f}\", \n",
    "        (logocv_temp.buffer_size[i], logocv_temp.NRMSE[i]), \n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10), \n",
    "        ha='center', \n",
    "        color='r', \n",
    "        fontsize=16\n",
    "    )\n",
    "\n",
    "# Set axes range and tick increments with larger fonts\n",
    "plt.xticks(np.arange(0, max(logocv_temp.buffer_size) + 1, 1), fontsize=14)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.yticks(np.arange(0, 1.1, 0.2), fontsize=16)\n",
    "\n",
    "# Legend placement in the top right corner with increased font size\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "plt.xlabel('Degrees Celsius buffering', fontsize=16)\n",
    "plt.ylabel('Coefficient of determination (R2)', fontsize=16)\n",
    "plt.title('', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
