{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapped sampling with replacement of trait prediction\n",
    "\n",
    "This Jupyter notebook resamples with replacement from the forest plot data and re-runs the predictive model n times, storing the results to calculate confidence intervals and coefficient of variation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-process and merge dfs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename CHELSA_bio columns consistently\n",
    "def rename_chelsa_columns(df):\n",
    "    def clean_chelsa_name(col):\n",
    "        if col.startswith('CHELSA_bio'):\n",
    "            parts = col.split('_')\n",
    "            if len(parts) > 1 and parts[1].startswith('bio'):\n",
    "                bio_number = parts[1][3:]  # Extract number part from 'bio<number>'\n",
    "                return f'CHELSA_bio{bio_number}'\n",
    "        return col\n",
    "\n",
    "    # Apply the renaming function to all columns\n",
    "    df.columns = [clean_chelsa_name(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# Function to get a consistent numerical order for CHELSA_bio columns\n",
    "def get_consistent_chelsa_order(*dfs):\n",
    "    # Collect all CHELSA_bio columns from all DataFrames\n",
    "    all_chelsa_columns = set()\n",
    "    for df in dfs:\n",
    "        chelsa_columns = [col for col in df.columns if col.startswith('CHELSA_bio')]\n",
    "        all_chelsa_columns.update(chelsa_columns)\n",
    "    \n",
    "    # Sort the collected columns numerically\n",
    "    sorted_chelsa_columns = sorted(all_chelsa_columns, key=lambda x: int(x.replace('CHELSA_bio', '')))\n",
    "    return sorted_chelsa_columns\n",
    "\n",
    "# Function to reorder columns based on a given order\n",
    "def reorder_columns(df, desired_order):\n",
    "    chelsa_columns = [col for col in df.columns if col.startswith('CHELSA_bio')]\n",
    "    non_chelsa_columns = [col for col in df.columns if not col.startswith('CHELSA_bio')]\n",
    "    \n",
    "    # Ensure that only the columns that exist in the DataFrame are reordered\n",
    "    chelsa_columns = [col for col in desired_order if col in chelsa_columns]\n",
    "    \n",
    "    # Reorder columns\n",
    "    ordered_columns = non_chelsa_columns + chelsa_columns\n",
    "    return df[ordered_columns]\n",
    "\n",
    "# Function to move 'lat' and 'lon' to specific positions\n",
    "def move_lat_lon_columns(df, lat_pos=None, lon_pos=None):\n",
    "    columns = df.columns.tolist()  # Get the list of columns\n",
    "    if 'lat' in columns and lat_pos is not None:\n",
    "        columns.insert(lat_pos, columns.pop(columns.index('lat')))  # Move 'lat' to the specified position\n",
    "    if 'lon' in columns and lon_pos is not None:\n",
    "        columns.insert(lon_pos, columns.pop(columns.index('lon')))  # Move 'lon' to the specified position\n",
    "    return df[columns]  # Reorder DataFrame\n",
    "\n",
    "# Load and rename columns for all DataFrames\n",
    "current_df = pd.read_csv('data/precomputed/plot_and_abiotic_data_current.csv')\n",
    "current_df = rename_chelsa_columns(current_df)\n",
    "\n",
    "future_climate_ssp126 = pd.read_csv('data/precomputed/plot_and_abiotic_data_ssp126.csv')\n",
    "future_climate_ssp126 = rename_chelsa_columns(future_climate_ssp126)\n",
    "\n",
    "future_climate_ssp370 = pd.read_csv('data/precomputed/plot_and_abiotic_data_ssp370.csv')\n",
    "future_climate_ssp370 = rename_chelsa_columns(future_climate_ssp370)\n",
    "\n",
    "future_climate_ssp585 = pd.read_csv('data/precomputed/plot_and_abiotic_data_ssp585.csv')\n",
    "future_climate_ssp585 = rename_chelsa_columns(future_climate_ssp585)\n",
    "future_climate_ssp585 = rename_chelsa_columns(future_climate_ssp585)\n",
    "\n",
    "# Determine the consistent CHELSA_bio column order\n",
    "desired_order = get_consistent_chelsa_order(current_df, future_climate_ssp126, future_climate_ssp370, future_climate_ssp585)\n",
    "\n",
    "# Apply the same column order to all DataFrames\n",
    "current_df = reorder_columns(current_df, desired_order)\n",
    "future_climate_ssp126 = reorder_columns(future_climate_ssp126, desired_order)\n",
    "future_climate_ssp370 = reorder_columns(future_climate_ssp370, desired_order)\n",
    "future_climate_ssp585 = reorder_columns(future_climate_ssp585, desired_order)\n",
    "\n",
    "# Move 'lat' and 'lon' columns to specific positions\n",
    "current_df = move_lat_lon_columns(current_df, lat_pos=26, lon_pos=27)\n",
    "future_climate_ssp126 = move_lat_lon_columns(future_climate_ssp126, lat_pos=1, lon_pos=2)  \n",
    "future_climate_ssp370 = move_lat_lon_columns(future_climate_ssp370, lat_pos=1, lon_pos=2)  \n",
    "future_climate_ssp585 = move_lat_lon_columns(future_climate_ssp585, lat_pos=1, lon_pos=2)  \n",
    "\n",
    "current_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrapping: Resample (with replacement) current forest plot data, retrain model and collect predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define traits and features\n",
    "trait_names = current_df.columns[1:25].tolist()\n",
    "climate_features = ['EarthEnvTopoMed_Eastness', 'EarthEnvTopoMed_Elevation',\n",
    "                    'EarthEnvTopoMed_Northness', 'EarthEnvTopoMed_Slope',\n",
    "                    'SG_Bulk_density_015cm', 'SG_Clay_Content_015cm',\n",
    "                    'SG_Coarse_fragments_015cm', 'SG_Depth_to_bedrock',\n",
    "                    'SG_Sand_Content_015cm', 'SG_Silt_Content_015cm', 'CHELSA_bio1',\n",
    "                    'CHELSA_bio7', 'CHELSA_bio9', 'CHELSA_bio11', 'CHELSA_bio12',\n",
    "                    'CHELSA_bio15', 'CHELSA_bio17', 'CHELSA_bio19']\n",
    "\n",
    "# Preprocessing tools\n",
    "scaler_features = StandardScaler()\n",
    "scaler_targets = StandardScaler()\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Preprocessing functions\n",
    "def preprocess_data(features_df, trait_names):\n",
    "    features = features_df[climate_features].values\n",
    "    features_scaled = scaler_features.fit_transform(features)\n",
    "    features_poly = poly.fit_transform(features_scaled)\n",
    "\n",
    "    traits = features_df[trait_names].values\n",
    "    traits_scaled = scaler_targets.fit_transform(traits)\n",
    "    \n",
    "    return features_poly, traits_scaled\n",
    "\n",
    "def preprocess_features(features_df):\n",
    "    features = features_df[climate_features].values\n",
    "    features_scaled = scaler_features.transform(features)\n",
    "    features_poly = poly.transform(features_scaled)\n",
    "    return features_poly\n",
    "\n",
    "def predict_traits(future_df, model):\n",
    "    # Prepare future climate data\n",
    "    features_poly = preprocess_features(future_df)\n",
    "    predicted_traits = multioutput_rf.predict(features_poly)\n",
    "    return pd.DataFrame(predicted_traits, columns=trait_names, index=future_df['pid'])\n",
    "\n",
    "# Split data\n",
    "features = current_df[climate_features].values\n",
    "traits = current_df[trait_names].values\n",
    "\n",
    "train_indices, test_indices, features_train, features_test, traits_train, traits_test = train_test_split(\n",
    "    current_df.index, features, traits, test_size=0.2, random_state=42, stratify=current_df['FIA_group']\n",
    ")\n",
    "\n",
    "# Preprocess train/test data\n",
    "features_train_scaled = scaler_features.fit_transform(features_train)\n",
    "features_test_scaled = scaler_features.transform(features_test)\n",
    "features_train_poly = poly.fit_transform(features_train_scaled)\n",
    "features_test_poly = poly.transform(features_test_scaled)\n",
    "traits_train_scaled = scaler_targets.fit_transform(traits_train)\n",
    "traits_test_scaled = scaler_targets.transform(traits_test)\n",
    "\n",
    "# Train the model\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    n_estimators=1800, random_state=42, n_jobs=-1, min_samples_split=2,\n",
    "    min_samples_leaf=1, max_features='log2', max_depth=20, bootstrap=False\n",
    ")\n",
    "\n",
    "multioutput_rf = MultiOutputRegressor(rf_regressor)\n",
    "multioutput_rf.fit(features_train_poly, traits_train_scaled)\n",
    "\n",
    "# Evaluate on the test set\n",
    "rf_pred_scaled = multioutput_rf.predict(features_test_poly)\n",
    "rf_r2_per_trait = np.array([r2_score(traits_test_scaled[:, i], rf_pred_scaled[:, i]) for i in range(len(trait_names))])\n",
    "rf_rmse_per_trait = np.sqrt(mean_squared_error(traits_test_scaled, rf_pred_scaled, multioutput='raw_values'))\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics_rf = pd.DataFrame({\n",
    "    'Trait': trait_names,\n",
    "    'RÂ²': rf_r2_per_trait,\n",
    "    'RMSE': rf_rmse_per_trait\n",
    "})\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(evaluation_metrics_rf)\n",
    "\n",
    "\n",
    "# Make plot-level predictions for each SSP across bootstrap runs\n",
    "\n",
    "n_bootstrap = 500  # Number of bootstrap iterations\n",
    "predicted_traits_bootstrap = {'SSP126': [], 'SSP370': [], 'SSP585': []}\n",
    "\n",
    "for i in tqdm(range(n_bootstrap), desc=\"Bootstrapping progress\"):\n",
    "    # Resample current data\n",
    "    stratified_sample = current_df.groupby('FIA_group').apply(\n",
    "        lambda x: x.sample(frac=1.0, replace=True, random_state=i)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Preprocess resampled data\n",
    "    boot_features = scaler_features.fit_transform(stratified_sample[climate_features].values)\n",
    "    boot_traits = scaler_targets.fit_transform(stratified_sample[trait_names].values)\n",
    "    boot_features_poly = poly.fit_transform(boot_features)\n",
    "\n",
    "    # Train model on resampled data\n",
    "    multioutput_rf = MultiOutputRegressor(rf_regressor)\n",
    "    multioutput_rf.fit(boot_features_poly, boot_traits)\n",
    "\n",
    "    # Predict traits for each SSP\n",
    "    for ssp, future_df in zip(['SSP126', 'SSP370', 'SSP585'], \n",
    "                              [future_climate_ssp126, future_climate_ssp370, future_climate_ssp585]):\n",
    "        pred = predict_traits(future_df, multioutput_rf)\n",
    "        \n",
    "        # Add a bootstrap_run column to keep track of each iteration\n",
    "        pred['bootstrap_run'] = i  \n",
    "        predicted_traits_bootstrap[ssp].append(pred)\n",
    "\n",
    "# Combine all bootstrap runs for each SSP scenario into one DataFrame\n",
    "predicted_all_runs_ssp126 = pd.concat(predicted_traits_bootstrap['SSP126'], \n",
    "                                      keys=range(n_bootstrap), \n",
    "                                      names=['bootstrap_run']).reset_index(level='bootstrap_run')\n",
    "predicted_all_runs_ssp370 = pd.concat(predicted_traits_bootstrap['SSP370'], \n",
    "                                      keys=range(n_bootstrap), \n",
    "                                      names=['bootstrap_run']).reset_index(level='bootstrap_run')\n",
    "predicted_all_runs_ssp585 = pd.concat(predicted_traits_bootstrap['SSP585'], \n",
    "                                      keys=range(n_bootstrap), \n",
    "                                      names=['bootstrap_run']).reset_index(level='bootstrap_run')\n",
    "\n",
    "# Add metadata to the combined DataFrames\n",
    "def add_metadata(predicted_df, source_df, ssp_label):\n",
    "    predicted_df = predicted_df.reset_index()  # Ensure 'pid' is a column\n",
    "    predicted_df['pid'] = predicted_df['pid'].astype(str)\n",
    "    source_df['pid'] = source_df['pid'].astype(str)\n",
    "    predicted_df = predicted_df.merge(\n",
    "        source_df[['pid', 'lat', 'lon', 'FIA_group', 'ECO_NAME', 'BIOME']], \n",
    "        on='pid'\n",
    "    )\n",
    "    predicted_df['SSP'] = ssp_label\n",
    "    return predicted_df\n",
    "\n",
    "predicted_all_runs_ssp126 = add_metadata(predicted_all_runs_ssp126, current_df, 'SSP126')\n",
    "predicted_all_runs_ssp370 = add_metadata(predicted_all_runs_ssp370, current_df, 'SSP370')\n",
    "predicted_all_runs_ssp585 = add_metadata(predicted_all_runs_ssp585, current_df, 'SSP585')\n",
    "\n",
    "# Save the full bootstrap predictions\n",
    "predicted_all_runs_ssp126.to_csv(\"predicted_traits_ssp126_all_bootstraps.csv\", index=False)\n",
    "predicted_all_runs_ssp370.to_csv(\"predicted_traits_ssp370_all_bootstraps.csv\", index=False)\n",
    "predicted_all_runs_ssp585.to_csv(\"predicted_traits_ssp585_all_bootstraps.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the bootstrap DataFrames for each SSP scenario \n",
    "predicted_all_runs_ssp126 = pd.concat(predicted_traits_bootstrap['SSP126'])\n",
    "predicted_all_runs_ssp370 = pd.concat(predicted_traits_bootstrap['SSP370'])\n",
    "predicted_all_runs_ssp585 = pd.concat(predicted_traits_bootstrap['SSP585'])\n",
    "\n",
    "# Reset the index to ensure 'pid' is a column rather than the index.\n",
    "# After reset_index, 'pid' will be a column again.\n",
    "predicted_all_runs_ssp126 = predicted_all_runs_ssp126.reset_index()\n",
    "predicted_all_runs_ssp370 = predicted_all_runs_ssp370.reset_index()\n",
    "predicted_all_runs_ssp585 = predicted_all_runs_ssp585.reset_index()\n",
    "\n",
    "# Add metadata\n",
    "predicted_all_runs_ssp126 = add_metadata(predicted_all_runs_ssp126, current_df, 'SSP126')\n",
    "predicted_all_runs_ssp370 = add_metadata(predicted_all_runs_ssp370, current_df, 'SSP370')\n",
    "predicted_all_runs_ssp585 = add_metadata(predicted_all_runs_ssp585, current_df, 'SSP585')\n",
    "\n",
    "# Save the full bootstrap predictions\n",
    "predicted_all_runs_ssp126.to_csv(\"predicted_traits_ssp126_all_bootstraps.csv\", index=False)\n",
    "predicted_all_runs_ssp370.to_csv(\"predicted_traits_ssp370_all_bootstraps.csv\", index=False)\n",
    "predicted_all_runs_ssp585.to_csv(\"predicted_traits_ssp585_all_bootstraps.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
